{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 1: Import Everything"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d56ab114114d56e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 2: Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cc3a801064b9d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filePath = \"Cleaned CCMatrix v1- EN to AR Dataset.tmx\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a27bd2dee1a1521"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_control_characters(chunk):\n",
    "    # Remove control characters except for tab, newline, and carriage return\n",
    "    chunk = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', chunk)\n",
    "    chunk = re.sub(r'\\ufffe', '', chunk)  # Remove the 0xFFFE character\n",
    "    return chunk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d86f2373e50d7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUFFER_SIZE_FILE = 1024 * 1024  # 1MB\n",
    "\n",
    "with open(\"CCMatrix v1- EN to AR Dataset.tmx\", mode='r', encoding='utf-8') as f_src, \\\n",
    "        open(filePath, mode='w', encoding='utf-8') as f_dst:\n",
    "    while True:\n",
    "        chunk = f_src.read(BUFFER_SIZE_FILE)\n",
    "        if not chunk:\n",
    "            break\n",
    "        cleaned_chunk = clean_control_characters(chunk)\n",
    "        f_dst.write(cleaned_chunk)\n",
    "print(\"Finished\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05481eb7cfba887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_tu_elements(tu):\n",
    "    ar_text = \"\"\n",
    "    en_text = \"\"\n",
    "    for tuv in tu.findall(\"tuv\"):\n",
    "        lang = tuv.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        seg_text = tuv.findtext(\"seg\")\n",
    "        if lang == \"ar\":\n",
    "            ar_text = seg_text\n",
    "        elif lang == \"en\":\n",
    "            en_text = seg_text\n",
    "    return ar_text, en_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8af487ce20d9ad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ar_texts = []\n",
    "en_texts = []\n",
    "\n",
    "counter = 0\n",
    "limit = 50000  # Change the number of sentences to read\n",
    "flag = True  # True, stop at limit. False, ignore limit\n",
    "\n",
    "context = etree.iterparse(filePath, events=('end',), tag='tu')\n",
    "for event, elem in context:\n",
    "    ar_text, en_text = extract_tu_elements(elem)\n",
    "    if ar_text != \"\" and en_text != \"\":\n",
    "        ar_texts.append(ar_text)\n",
    "        en_texts.append(en_text)\n",
    "        counter += 1\n",
    "    # clear the element to free up memory\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "    if flag and counter == limit:\n",
    "        break\n",
    "print(\"Arabic:\", len(ar_texts))\n",
    "print(\"English:\", len(en_texts))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579e5cc8a63dd33b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a500178240c5c1eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer_en.fit_on_texts(en_texts)\n",
    "word_index_en = tokenizer_en.word_index\n",
    "\n",
    "tokenizer_ar = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer_ar.fit_on_texts(ar_texts)\n",
    "word_index_ar = tokenizer_ar.word_index\n",
    "#tokenizer.fit_on_texts(data_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47370023edb553b9"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22428\n",
      "59080\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE_EN = len(word_index_en) + 2\n",
    "print(VOCAB_SIZE_EN)\n",
    "\n",
    "VOCAB_SIZE_AR = len(word_index_ar) + 2\n",
    "print(VOCAB_SIZE_AR)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T05:59:38.998112600Z",
     "start_time": "2023-10-24T05:59:38.994112Z"
    }
   },
   "id": "ef95dfac2e4f1d24"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "START_TOKEN_EN = VOCAB_SIZE_EN - 2\n",
    "END_TOKEN_EN = VOCAB_SIZE_EN - 1\n",
    "inputs = [[START_TOKEN_EN] + tokenizer_en.texts_to_sequences([sentence])[0] + [END_TOKEN_EN] for sentence in en_texts]\n",
    "\n",
    "START_TOKEN_AR = VOCAB_SIZE_AR - 2\n",
    "END_TOKEN_AR = VOCAB_SIZE_AR - 1\n",
    "\n",
    "outputs = [[START_TOKEN_AR] + tokenizer_en.texts_to_sequences([sentence])[0] + [END_TOKEN_AR] for sentence in en_texts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:06:23.400387300Z",
     "start_time": "2023-10-24T06:06:22.691058600Z"
    }
   },
   "id": "7be33e5251851840"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the tokenized data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eb7e651154448e3"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22426, 38, 2, 11709, 5, 2, 654, 8906, 11710, 4348, 30, 1597, 22427], [22426, 11, 108, 38, 13, 7573, 22427], [22426, 95, 5, 2, 600, 31, 663, 1230, 144, 55, 6, 103, 41, 4, 57, 4127, 2, 4594, 22427], [22426, 1388, 76, 11, 4349, 52, 155, 22427], [22426, 2, 601, 56, 3770, 36, 898, 375, 505, 13, 22427]]\n",
      "[[59078, 38, 2, 11709, 5, 2, 654, 8906, 11710, 4348, 30, 1597, 59079], [59078, 11, 108, 38, 13, 7573, 59079], [59078, 95, 5, 2, 600, 31, 663, 1230, 144, 55, 6, 103, 41, 4, 57, 4127, 2, 4594, 59079], [59078, 1388, 76, 11, 4349, 52, 155, 59079], [59078, 2, 601, 56, 3770, 36, 898, 375, 505, 13, 59079]]\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:5])\n",
    "print(outputs[:5])\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:13:05.534322600Z",
     "start_time": "2023-10-24T06:13:05.527773300Z"
    }
   },
   "id": "6dd1e78311f02513"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove long sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2e49d9552512f7d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43099\n",
      "43099\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "indices_to_remove = [indx for indx, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "# Remove from the last, since doing it in the normal way would fuck up the length making the indices shift by one to the left, so deleting from the right is safe\n",
    "for idx in reversed(indices_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "# do the same but for arabic    \n",
    "indices_to_remove = [indx for indx, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(indices_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:14:18.245994600Z",
     "start_time": "2023-10-24T06:14:18.235219700Z"
    }
   },
   "id": "228c8ce6963fcb12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input/Output Creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec364b7e38aca399"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:34:30.441894600Z",
     "start_time": "2023-10-24T06:34:30.341260500Z"
    }
   },
   "id": "3ac800d51cacc095"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "datasets = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "datasets = datasets.cache()  # Speed training, but does nothing else kek\n",
    "datasets = datasets.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "datasets = datasets.prefetch(tf.data.experimental.AUTOTUNE)  # Speed training, but does nothing else kek\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:40:18.140893900Z",
     "start_time": "2023-10-24T06:40:16.087565200Z"
    }
   },
   "id": "5cc646a144e0b3c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 3: Model Building"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eebc57023fa5aae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48bd6614a80d7e01"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):  # pos is (seq_ength,1) and i is (1,d_model), hence the return \n",
    "        # pos and i are arrays\n",
    "        angles = 1 / np.power(10000., (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angles  # returns (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])  # even\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])  # odd\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:54:43.765542400Z",
     "start_time": "2023-10-24T06:54:43.762813400Z"
    }
   },
   "id": "761720deeaa94b4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "beb5b659b3a8e3c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
