{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d56ab114114d56e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 1: Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:23.283262600Z",
     "start_time": "2023-10-28T11:48:20.396931200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-28 14:48:20.922929: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-28 14:48:20.922970: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-28 14:48:20.924534: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1: INFO, 2: WARNING, 3: ERROR)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Hide any TensorFlow warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "from lxml import etree\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc3a801064b9d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceeaf99c-0f7c-4383-aec9-f2f4d5a08026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:27.449894300Z",
     "start_time": "2023-10-28T11:48:27.396895300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8af487ce20d9ad3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:28.509186400Z",
     "start_time": "2023-10-28T11:48:28.452189100Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tu_elements(tu):\n",
    "    ar_text = \"\"\n",
    "    en_text = \"\"\n",
    "    for tuv in tu.findall(\"tuv\"):\n",
    "        lang = tuv.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        seg_text = tuv.findtext(\"seg\")\n",
    "        if lang == \"ar\":\n",
    "            ar_text = seg_text\n",
    "        elif lang == \"en\":\n",
    "            en_text = seg_text\n",
    "    return ar_text, en_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579e5cc8a63dd33b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:32.944452600Z",
     "start_time": "2023-10-28T11:48:31.586998500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3105556964874268 seconds\n",
      "Arabic: 100000\n",
      "English: 100000\n"
     ]
    }
   ],
   "source": [
    "filePath = \"Cleaned CCMatrix v1- EN to AR Dataset.tmx\"\n",
    "start = time.time()\n",
    "ar_texts = []\n",
    "en_texts = []\n",
    "\n",
    "counter = 0\n",
    "limit = 100000  # Change the number of sentences to read\n",
    "#limit = 50000  # Change the number of sentences to read\n",
    "\n",
    "context = etree.iterparse(filePath, events=('end',), tag='tu')\n",
    "for event, elem in context:\n",
    "    ar_text, en_text = extract_tu_elements(elem)\n",
    "    if ar_text != \"\" and en_text != \"\":\n",
    "        ar_texts.append(ar_text)\n",
    "        en_texts.append(en_text)\n",
    "        counter += 1\n",
    "    # clear the element to free up memory\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "    if counter == limit:\n",
    "        break\n",
    "end = time.time()\n",
    "print(f\"{end - start} seconds\")\n",
    "print(\"Arabic:\", len(ar_texts))\n",
    "print(\"English:\", len(en_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    # Remove periods, brackets, quotes, asterisks, and other special characters\n",
    "    corpus = re.sub(r'[.*[\\]()\"“”]', '', corpus)\n",
    "\n",
    "    # Collapses multiple spaces into single spaces\n",
    "    corpus = re.sub(r\"  +\", ' ', corpus)\n",
    "\n",
    "    # Split the string into lines wherever there's a newline\n",
    "    return corpus.split(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:34.025574200Z",
     "start_time": "2023-10-28T11:48:33.976517100Z"
    }
   },
   "id": "4ed974472d2baad6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic (after preprocessing): 100000\n",
      "English (after preprocessing): 100000\n"
     ]
    }
   ],
   "source": [
    "ar_texts = preprocess_corpus(\"\\n\".join(ar_texts))\n",
    "en_texts = preprocess_corpus(\"\\n\".join(en_texts))\n",
    "\n",
    "# Print the lengths again after preprocessing\n",
    "print(\"Arabic (after preprocessing):\", len(ar_texts))\n",
    "print(\"English (after preprocessing):\", len(en_texts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:36.474962600Z",
     "start_time": "2023-10-28T11:48:36.249361300Z"
    }
   },
   "id": "574536bc51a30258"
  },
  {
   "cell_type": "markdown",
   "id": "a500178240c5c1eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309aea77-eb47-42ea-8e82-63acaa2a34f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Ignore this if you do not have a tokenizer to load\n",
    "# # Loading the English tokenizer\n",
    "# with open('tokenizer_en.pkl', 'rb') as handle:\n",
    "#     tokenizer_en = pickle.load(handle)\n",
    "#     word_index_en = tokenizer_en.word_index\n",
    "# \n",
    "# # Loading the Arabic tokenizer\n",
    "# with open('tokenizer_ar.pkl', 'rb') as handle:\n",
    "#     tokenizer_ar = pickle.load(handle)\n",
    "#     word_index_ar = tokenizer_ar.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47370023edb553b9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:44.802413400Z",
     "start_time": "2023-10-28T11:48:43.117909800Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer( oov_token='<OOV>', lower=True)\n",
    "tokenizer_en.fit_on_texts(en_texts)\n",
    "word_index_en = tokenizer_en.word_index\n",
    "\n",
    "tokenizer_ar = Tokenizer(oov_token='<OOV>', lower=True)\n",
    "tokenizer_ar.fit_on_texts(ar_texts)\n",
    "word_index_ar = tokenizer_ar.word_index\n",
    "#tokenizer.fit_on_texts(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "142f8692-9c25-426f-8272-441641854ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:45.630110900Z",
     "start_time": "2023-10-28T11:48:45.537398500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the English tokenizer\n",
    "with open('tokenizer_en.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_en, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Saving the Arabic tokenizer\n",
    "with open('tokenizer_ar.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_ar, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef95dfac2e4f1d24",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T11:48:47.026230100Z",
     "start_time": "2023-10-28T11:48:47.020230500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32002\n",
      "89396\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE_EN = len(word_index_en) + 2\n",
    "print(VOCAB_SIZE_EN)\n",
    "\n",
    "VOCAB_SIZE_AR = len(word_index_ar) + 2\n",
    "print(VOCAB_SIZE_AR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be33e5251851840",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "START_TOKEN_EN = VOCAB_SIZE_EN - 2\n",
    "END_TOKEN_EN = VOCAB_SIZE_EN - 1\n",
    "inputs = [[START_TOKEN_EN] + tokenizer_en.texts_to_sequences([sentence])[0] + [END_TOKEN_EN] for sentence in en_texts]\n",
    "# Clean from memory\n",
    "del en_texts\n",
    "\n",
    "START_TOKEN_AR = VOCAB_SIZE_AR - 2\n",
    "END_TOKEN_AR = VOCAB_SIZE_AR - 1\n",
    "\n",
    "outputs = [[START_TOKEN_AR] + tokenizer_ar.texts_to_sequences([sentence])[0] + [END_TOKEN_AR] for sentence in ar_texts]\n",
    "# Clean from memory\n",
    "del ar_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7e651154448e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1e78311f02513",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(inputs[:5])\n",
    "print(outputs[:5])\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e49d9552512f7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Remove long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c8ce6963fcb12",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "indices_to_remove = [indx for indx, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "# Remove from the last, since doing it in the normal way would fuck up the length making the indices shift by one to the left, so deleting from the right is safe\n",
    "for idx in reversed(indices_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "# do the same but for arabic    \n",
    "indices_to_remove = [indx for indx, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(indices_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec364b7e38aca399",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Input/Output Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac800d51cacc095",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding=\"post\",\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding=\"post\",\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc646a144e0b3c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eebc57023fa5aae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3: Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd6614a80d7e01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761720deeaa94b4c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):  # pos: (seq_length, 1) i: (1, d_model)\n",
    "        angles = 1 / np.power(10000., (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angles  # (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5373f99339abe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14997e2e70b9283",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5b659b3a8e3c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "\n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "    def split_proj(self, inputs, batch_size):  # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape)  # (batch_size, seq_length, nb_proj, d_proj)\n",
    "\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])  # (batch_size, nb_proj, seq_length, d_proj)\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a73a688a54681",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553ee4885c73854",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FFN_units, nb_proj, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9875a50cf0542c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout)\n",
    "                           for _ in range(nb_layers)]\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104b7abcf33105d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb0d7674bb647a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FFN_units, nb_proj, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training=training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "\n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training=training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595defd0d2ac9d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout)\n",
    "                           for _ in range(nb_layers)]\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824002cb63581dc8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89842a5ab2476f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
    "\n",
    "    def create_padding_mask(self, seq):  # seq: (batch_size, seq_length)\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "\n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "\n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f93b8a27533da5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488092cb88478253",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 512  # 512\n",
    "NB_LAYERS = 4  # 6\n",
    "FFN_UNITS = 2048  # 2048\n",
    "NB_PROJ = 8  # 8\n",
    "DROPOUT = 0.1  # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_AR,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301473076f7fd304",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2334f121e72b33c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)  # Cast step to float32\n",
    "\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b4fac76e0d777",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./MODEL\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446ecd440c876c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "EPOCHS = 15\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch + 1))\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpont for epoch {} at {}\".format(epoch + 1, ckpt_save_path))\n",
    "    print(\"time taken for 1 epoch: {} secs\\n\".format(time.time() - start))\n",
    "    losses.append(train_loss.result())\n",
    "    accuracies.append(train_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.title('Training Loss across Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Training Accuracy', color='r')\n",
    "plt.title('Training Accuracy across Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bef62d7938099c0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da8eabb41a3e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    # Tokenize the input sentence\n",
    "    inp_sequence = tokenizer_en.texts_to_sequences([inp_sentence])[0]\n",
    "    inp_sentence = [VOCAB_SIZE_EN - 2] + inp_sequence + [VOCAB_SIZE_EN - 1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_AR - 2], axis=0)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False)  # (1, seq_length, vocab_size_ar)\n",
    "\n",
    "        prediction = predictions[:, -1:, :]\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == VOCAB_SIZE_AR - 1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa53a25e08fc29",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    # Decode the sequence back to text\n",
    "    predicted_sentence = tokenizer_ar.sequences_to_texts([output])[0]\n",
    "    # Split the predicted sentence into words and skip the first word, cause it is attached for everything\n",
    "    words = predicted_sentence.split()[1:]\n",
    "\n",
    "    # Join the words back into a sentence\n",
    "    modified_predicted_sentence = ' '.join(words)\n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted translation: {}\".format(modified_predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970560bf719ba15",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(\"Why am i so bad \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3c76d6f2f8000baf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
