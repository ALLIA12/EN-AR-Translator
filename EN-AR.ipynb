{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 1: Import Everything"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d56ab114114d56e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 2: Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cc3a801064b9d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_control_characters(chunk):\n",
    "    # Remove control characters except for tab, newline, and carriage return\n",
    "    chunk = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', chunk)\n",
    "    chunk = re.sub(r'\\ufffe', '', chunk)  # Remove the 0xFFFE character\n",
    "    return chunk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d86f2373e50d7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1024 * 1024  # 1MB\n",
    "\n",
    "start_time = time.time()\n",
    "filePath = \"Cleaned CCMatrix v1- EN to AR Dataset.tmx\"\n",
    "with open(\"CCMatrix v1- EN to AR Dataset.tmx\", mode='r', encoding='utf-8') as f_src, \\\n",
    "     open(filePath, mode='w', encoding='utf-8') as f_dst:\n",
    "\n",
    "    while True:\n",
    "        chunk = f_src.read(BUFFER_SIZE)\n",
    "        if not chunk:\n",
    "            break\n",
    "        cleaned_chunk = clean_control_characters(chunk)\n",
    "        f_dst.write(cleaned_chunk)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "print(f\"Time taken to process the file: {time_taken:.2f} seconds\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05481eb7cfba887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_tu_elements(tu):\n",
    "    ar_text = \"\"\n",
    "    en_text = \"\"\n",
    "    for tuv in tu.findall(\"tuv\"):\n",
    "        lang = tuv.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        seg_text = tuv.findtext(\"seg\")\n",
    "        if lang == \"ar\":\n",
    "            ar_text = seg_text\n",
    "        elif lang == \"en\":\n",
    "            en_text = seg_text\n",
    "    return ar_text, en_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8af487ce20d9ad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "\n",
    "ar_texts = []\n",
    "en_texts = []\n",
    "\n",
    "counter = 0\n",
    "limit = 5000000  # Change the number of words to read\n",
    "flag = False  # True, stop at limit. False, ignore limit\n",
    "\n",
    "start_time = time.time()  # Record start time\n",
    "context = etree.iterparse(filePath, events=('end',), tag='tu')\n",
    "for event, elem in context:\n",
    "    ar_text, en_text = extract_tu_elements(elem)\n",
    "    if ar_text != \"\" and en_text != \"\":\n",
    "        ar_texts.append(ar_text)\n",
    "        en_texts.append(en_text)\n",
    "        counter += 1\n",
    "    # clear the element to free up memory\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "    if flag and counter == limit:\n",
    "        break\n",
    "end_time = time.time()  # Record end time\n",
    "print(\"Time taken to parse and extract: {:.2f} seconds\".format(end_time - start_time))\n",
    "\n",
    "print(\"Arabic:\", len(ar_texts))\n",
    "print(\"English:\", len(en_texts))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579e5cc8a63dd33b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2 ** 16, oov_token='<OOV>')\n",
    "#tokenizer.fit_on_texts(data_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47370023edb553b9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "48bd6614a80d7e01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
