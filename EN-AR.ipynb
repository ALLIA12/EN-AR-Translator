{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 1: Import Everything"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d56ab114114d56e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from lxml import etree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 2: Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cc3a801064b9d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filePath = \"Cleaned CCMatrix v1- EN to AR Dataset.tmx\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a27bd2dee1a1521"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_control_characters(chunk):\n",
    "    # Remove control characters except for tab, newline, and carriage return\n",
    "    chunk = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]', '', chunk)\n",
    "    chunk = re.sub(r'\\ufffe', '', chunk)  # Remove the 0xFFFE character\n",
    "    return chunk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d86f2373e50d7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUFFER_SIZE_FILE = 1024 * 1024  # 1MB\n",
    "\n",
    "with open(\"CCMatrix v1- EN to AR Dataset.tmx\", mode='r', encoding='utf-8') as f_src, \\\n",
    "        open(filePath, mode='w', encoding='utf-8') as f_dst:\n",
    "    while True:\n",
    "        chunk = f_src.read(BUFFER_SIZE_FILE)\n",
    "        if not chunk:\n",
    "            break\n",
    "        cleaned_chunk = clean_control_characters(chunk)\n",
    "        f_dst.write(cleaned_chunk)\n",
    "print(\"Finished\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05481eb7cfba887"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_tu_elements(tu):\n",
    "    ar_text = \"\"\n",
    "    en_text = \"\"\n",
    "    for tuv in tu.findall(\"tuv\"):\n",
    "        lang = tuv.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        seg_text = tuv.findtext(\"seg\")\n",
    "        if lang == \"ar\":\n",
    "            ar_text = seg_text\n",
    "        elif lang == \"en\":\n",
    "            en_text = seg_text\n",
    "    return ar_text, en_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8af487ce20d9ad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ar_texts = []\n",
    "en_texts = []\n",
    "\n",
    "counter = 0\n",
    "limit = 50000  # Change the number of sentences to read\n",
    "flag = True  # True, stop at limit. False, ignore limit\n",
    "\n",
    "context = etree.iterparse(filePath, events=('end',), tag='tu')\n",
    "for event, elem in context:\n",
    "    ar_text, en_text = extract_tu_elements(elem)\n",
    "    if ar_text != \"\" and en_text != \"\":\n",
    "        ar_texts.append(ar_text)\n",
    "        en_texts.append(en_text)\n",
    "        counter += 1\n",
    "    # clear the element to free up memory\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "    if flag and counter == limit:\n",
    "        break\n",
    "print(\"Arabic:\", len(ar_texts))\n",
    "print(\"English:\", len(en_texts))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579e5cc8a63dd33b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a500178240c5c1eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer_en.fit_on_texts(en_texts)\n",
    "word_index_en = tokenizer_en.word_index\n",
    "\n",
    "tokenizer_ar = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer_ar.fit_on_texts(ar_texts)\n",
    "word_index_ar = tokenizer_ar.word_index\n",
    "#tokenizer.fit_on_texts(data_clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47370023edb553b9"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22428\n",
      "59080\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE_EN = len(word_index_en) + 2\n",
    "print(VOCAB_SIZE_EN)\n",
    "\n",
    "VOCAB_SIZE_AR = len(word_index_ar) + 2\n",
    "print(VOCAB_SIZE_AR)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T05:59:38.998112600Z",
     "start_time": "2023-10-24T05:59:38.994112Z"
    }
   },
   "id": "ef95dfac2e4f1d24"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "START_TOKEN_EN = VOCAB_SIZE_EN - 2\n",
    "END_TOKEN_EN = VOCAB_SIZE_EN - 1\n",
    "inputs = [[START_TOKEN_EN] + tokenizer_en.texts_to_sequences([sentence])[0] + [END_TOKEN_EN] for sentence in en_texts]\n",
    "\n",
    "START_TOKEN_AR = VOCAB_SIZE_AR - 2\n",
    "END_TOKEN_AR = VOCAB_SIZE_AR - 1\n",
    "\n",
    "outputs = [[START_TOKEN_AR] + tokenizer_en.texts_to_sequences([sentence])[0] + [END_TOKEN_AR] for sentence in en_texts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:06:23.400387300Z",
     "start_time": "2023-10-24T06:06:22.691058600Z"
    }
   },
   "id": "7be33e5251851840"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the tokenized data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eb7e651154448e3"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22426, 38, 2, 11709, 5, 2, 654, 8906, 11710, 4348, 30, 1597, 22427], [22426, 11, 108, 38, 13, 7573, 22427], [22426, 95, 5, 2, 600, 31, 663, 1230, 144, 55, 6, 103, 41, 4, 57, 4127, 2, 4594, 22427], [22426, 1388, 76, 11, 4349, 52, 155, 22427], [22426, 2, 601, 56, 3770, 36, 898, 375, 505, 13, 22427]]\n",
      "[[59078, 38, 2, 11709, 5, 2, 654, 8906, 11710, 4348, 30, 1597, 59079], [59078, 11, 108, 38, 13, 7573, 59079], [59078, 95, 5, 2, 600, 31, 663, 1230, 144, 55, 6, 103, 41, 4, 57, 4127, 2, 4594, 59079], [59078, 1388, 76, 11, 4349, 52, 155, 59079], [59078, 2, 601, 56, 3770, 36, 898, 375, 505, 13, 59079]]\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(inputs[:5])\n",
    "print(outputs[:5])\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:13:05.534322600Z",
     "start_time": "2023-10-24T06:13:05.527773300Z"
    }
   },
   "id": "6dd1e78311f02513"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove long sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2e49d9552512f7d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43099\n",
      "43099\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "indices_to_remove = [indx for indx, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "# Remove from the last, since doing it in the normal way would fuck up the length making the indices shift by one to the left, so deleting from the right is safe\n",
    "for idx in reversed(indices_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "# do the same but for arabic    \n",
    "indices_to_remove = [indx for indx, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(indices_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:14:18.245994600Z",
     "start_time": "2023-10-24T06:14:18.235219700Z"
    }
   },
   "id": "228c8ce6963fcb12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input/Output Creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec364b7e38aca399"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:34:30.441894600Z",
     "start_time": "2023-10-24T06:34:30.341260500Z"
    }
   },
   "id": "3ac800d51cacc095"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "datasets = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "datasets = datasets.cache()  # Speed training, but does nothing else kek\n",
    "datasets = datasets.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "datasets = datasets.prefetch(tf.data.experimental.AUTOTUNE)  # Speed training, but does nothing else kek\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:40:18.140893900Z",
     "start_time": "2023-10-24T06:40:16.087565200Z"
    }
   },
   "id": "5cc646a144e0b3c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 3: Model Building"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eebc57023fa5aae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48bd6614a80d7e01"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):  # pos is (seq_ength,1) and i is (1,d_model), hence the return \n",
    "        # pos and i are arrays\n",
    "        angles = 1 / np.power(10000., (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angles  # returns (seq_length, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])  # even\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])  # odd\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T06:54:43.765542400Z",
     "start_time": "2023-10-24T06:54:43.762813400Z"
    }
   },
   "id": "761720deeaa94b4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d5373f99339abe"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "\n",
    "    return attention"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T07:21:15.031979400Z",
     "start_time": "2023-10-24T07:21:15.021785500Z"
    }
   },
   "id": "e14997e2e70b9283"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, nb_projection):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_projection = nb_projection\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_projection == 0\n",
    "\n",
    "        self.d_proj = self.d_model // self.nb_projection\n",
    "\n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "    def split_proj(self, inputs, batch_size):  # inputs: (batch_size,seq_length,d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_projection,\n",
    "                 self.d_proj)\n",
    "        splitted_inputs = tf.reshape(inputs, shape=shape)  #(batch_size, seq_length, nb_proj, d_proj)\n",
    "\n",
    "        return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3])  # (batch_size, nb_proj, seq_length, d_proj)\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "\n",
    "        outputs_att = self.final_lin(concat_attention)\n",
    "\n",
    "        return outputs_att"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T07:34:39.571118300Z",
     "start_time": "2023-10-24T07:34:39.530824100Z"
    }
   },
   "id": "beb5b659b3a8e3c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3a73a688a54681"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, FFN_units, nb_proj, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, mask, training=False):\n",
    "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T07:47:03.063984400Z",
     "start_time": "2023-10-24T07:47:03.060985200Z"
    }
   },
   "id": "4553ee4885c73854"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name='encoder'):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout) for _ in range(nb_layers)]\n",
    "\n",
    "    def call(self, inputs, mask, training=False):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T07:58:09.218654700Z",
     "start_time": "2023-10-24T07:58:09.206654400Z"
    }
   },
   "id": "a9875a50cf0542c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5104b7abcf33105d"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.m_model = input_shape[-1]\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.m_model, activation='relu')\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training=False):\n",
    "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "\n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:20:44.536813400Z",
     "start_time": "2023-10-24T10:20:44.495245700Z"
    }
   },
   "id": "cadb0d7674bb647a"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_projc,\n",
    "                 droupout,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name='decoder'):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.post_encoding = PositionalEncoding()\n",
    "        self.dropout_1 = layers.Dropout(rate=droupout)\n",
    "\n",
    "        self.decoder_layers = [DecoderLayer(FFN_units, nb_projc, droupout) for _ in range(nb_layers)]\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.post_encoding(outputs)\n",
    "        outputs = self.dropout_1(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.decoder_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:20:45.557232400Z",
     "start_time": "2023-10-24T10:20:45.552699400Z"
    }
   },
   "id": "5595defd0d2ac9d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "824002cb63581dc8"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout,\n",
    "                 name='transformer'):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_enc, d_model)\n",
    "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_dec, d_model)\n",
    "\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
    "\n",
    "    def create_padding_mask(self, seq):  # seq = (batch_size, seq_length)\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "\n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
    "                                self.create_look_ahead_mask(dec_inputs)\n",
    "                                )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
    "\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "\n",
    "        return outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:20:49.592423300Z",
     "start_time": "2023-10-24T10:20:49.581357700Z"
    }
   },
   "id": "89842a5ab2476f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stage 4: Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3f93b8a27533da5"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# Hyper-Parameters\n",
    "D_MODEL = 128  # 512\n",
    "NB_LAYERS = 4  # 6\n",
    "FFN_UNITS = 512  # 2048\n",
    "NB_PROJ = 8  # 8\n",
    "DROPOUT = 0.1  # 0.1\n",
    "\n",
    "transfomer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                         vocab_size_dec=VOCAB_SIZE_AR,\n",
    "                         d_model=D_MODEL,\n",
    "                         nb_layers=NB_LAYERS,\n",
    "                         FFN_units=FFN_UNITS,\n",
    "                         nb_proj=NB_PROJ,\n",
    "                         dropout=DROPOUT,\n",
    "                         )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:20:50.052558300Z",
     "start_time": "2023-10-24T10:20:50.007871300Z"
    }
   },
   "id": "488092cb88478253"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:20:50.329124500Z",
     "start_time": "2023-10-24T10:20:50.321428600Z"
    }
   },
   "id": "301473076f7fd304"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:20:50.640914700Z",
     "start_time": "2023-10-24T10:20:50.633027Z"
    }
   },
   "id": "e2334f121e72b33c"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "checkpoint_path = \"./MODEL\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transfomer=transfomer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:38:53.977301400Z",
     "start_time": "2023-10-24T10:38:53.966839500Z"
    }
   },
   "id": "7b0b4fac76e0d777"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting of epoc1\n",
      "Epoch 1 Batch 0 Loss 4.1729 Accuracy0.1308\n",
      "Saving checkpoint for epoch 1 at ./MODEL\\ckpt-1\n",
      "Time taken for 1 epoch  3.6235220432281494 seconds \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Starting of epoc{epoch + 1}\")\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(datasets):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = transfomer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, transfomer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transfomer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, prediction)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy{:.4f}\".format(epoch + 1, batch, train_loss.result(),\n",
    "                                                                        train_accuracy.result()))\n",
    "            break\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f\"Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}\")\n",
    "    print(f\"Time taken for 1 epoch  {time.time() - start} seconds \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:38:58.059937600Z",
     "start_time": "2023-10-24T10:38:54.432607Z"
    }
   },
   "id": "3446ecd440c876c7"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = [START_TOKEN_EN] + tokenizer_en.texts_to_sequences([inp_sentence])[0] + [END_TOKEN_EN]\n",
    "    enc_inputs = tf.expand_dims(inp_sentence, axis=0)\n",
    "    outputs = tf.expand_dims([START_TOKEN_AR], axis=0)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transfomer(enc_inputs, outputs, False)  # (1, Seq Length, vocab_size_ar)\n",
    "        prediction = predictions[:, -1, :]\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        if predicted_id == END_TOKEN_AR:\n",
    "            return tf.squeeze(outputs, axis=0)\n",
    "        predicted_id = tf.expand_dims(predicted_id, -1)  # Expand dimensions to make it [1,1]\n",
    "        outputs = tf.concat([outputs, predicted_id], axis=-1)\n",
    "    return tf.squeeze(outputs, axis=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:45:17.356387800Z",
     "start_time": "2023-10-24T10:45:17.307821Z"
    }
   },
   "id": "19da8eabb41a3e2"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    outputs = evaluate(sentence).numpy()\n",
    "    filtered_outputs = [i for i in outputs if i < START_TOKEN_AR]\n",
    "\n",
    "    # Decode the filtered token IDs back to text\n",
    "    decoded_text = tokenizer_ar.sequences_to_texts([filtered_outputs])[0]\n",
    "    print(f\"Input: {sentence}, Output: {decoded_text}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:45:17.822320400Z",
     "start_time": "2023-10-24T10:45:17.815320800Z"
    }
   },
   "id": "cdaa53a25e08fc29"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Why Not, Output: لا لا\n"
     ]
    }
   ],
   "source": [
    "translate(\"Why Not\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T10:45:47.238010Z",
     "start_time": "2023-10-24T10:45:47.026962400Z"
    }
   },
   "id": "1970560bf719ba15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed80f0001cfdac5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
